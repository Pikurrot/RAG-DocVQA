{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_documents(directory, limit=None):\n",
    "    documents = []\n",
    "    filenames = os.listdir(directory) if limit is None else os.listdir(directory)[:limit]\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory, filename), 'r') as f:\n",
    "                documents.append(f.read())\n",
    "    return documents\n",
    "\n",
    "documents = load_documents(\"/home/eric/Documents/CVC_Internship/data/spdocvqa_ocr_txt\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/miniconda3/envs/computer_vision_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, GenerationConfig\n",
    "\n",
    "# Choose your model\n",
    "model_name = 'gpt2'  # Replace with your chosen model\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set the generation configuration\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=512,      # Adjust as needed (input + output length)\n",
    "    max_new_tokens=50,   # Adjust as needed (output length)\n",
    "    # You can set other generation parameters here\n",
    ")\n",
    "model.generation_config = generation_config\n",
    "\n",
    "# Create a text-generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load an embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
    "\n",
    "# Create the vector store directly from texts and embeddings\n",
    "vectorstore = FAISS.from_texts(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# Create an LLM wrapper for LangChain\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generator,\n",
    "    model_kwargs={\"max_length\": 512, \"max_new_tokens\": 50}\n",
    ")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Create the RAG chain using from_chain_type\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Options: \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter to be written\\nby Dr. Shank\\n\"NUTRITION PRINCIPLES\"\\nSource: https://www.industrydocuments.ucsf.edu/docs/mhbf0227\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who wrote the chapter of NUTRITION PRINCIPLES?\n",
      "Answer: {'query': 'Who wrote the chapter of NUTRITION PRINCIPLES?', 'result': 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nChapter to be written\\nby Dr. Shank\\n\"NUTRITION PRINCIPLES\"\\nSource: https://www.industrydocuments.ucsf.edu/docs/mhbf0227\\n\\n\\nQuestion: Who wrote the chapter of NUTRITION PRINCIPLES?\\nHelpful Answer: The book itself does not cover NUTRIATION. The book is written by Dr. Shank, the author of NUTRITION PRINCIPLES, of which I am the publisher. I will post the book on my personal Facebook page.'}\n"
     ]
    }
   ],
   "source": [
    "# Your query\n",
    "query = \"Who wrote the chapter of NUTRITION PRINCIPLES?\"\n",
    "\n",
    "# Get the answer from the RAG system\n",
    "answer = rag_chain.invoke(query)\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
